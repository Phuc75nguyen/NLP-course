{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download dữ liệu cần thiết từ NLTK (SemCor và WordNet)\n",
    "nltk.download('semcor')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_instances(target_lemma):\n",
    "    \"\"\"\n",
    "    Trích xuất các instance từ SemCor cho từ mục tiêu (target_lemma).\n",
    "    Mỗi instance gồm:\n",
    "      - Đặc trưng được trích xuất từ ngữ cảnh gồm 2 từ trước và 2 từ sau,\n",
    "        bao gồm cả POS tags và 2 bigram (các cặp từ liên tiếp).\n",
    "      - Nhãn: tên synset của từ (là nhãn của nghĩa được gán).\n",
    "    \n",
    "    Cụ thể, các đặc trưng được xây dựng theo cách:\n",
    "      * Các từ và POS ở vị trí i-2, i-1, i+1, i+2.\n",
    "      * Các bigram: (i-2, i-1) và (i+1, i+2).\n",
    "    Các từ gần vị trí cần phân giải (i-1, i+1) được gán trọng số cao hơn.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    # Duyệt qua từng câu có gán nhãn trong SemCor\n",
    "    for sent in semcor.tagged_sents(tag='both'):\n",
    "        # Đảm bảo có đủ ngữ cảnh: bắt đầu từ vị trí thứ 2 đến len(sent)-2\n",
    "        for i in range(2, len(sent) - 2):\n",
    "            word_tree = sent[i]\n",
    "            if isinstance(word_tree, nltk.Tree):\n",
    "                # Lấy nhãn nghĩa của từ; lưu ý: cách gán nhãn phụ thuộc vào cấu trúc của SemCor\n",
    "                sense_tag = word_tree.label()\n",
    "                # Kiểm tra xem sense_tag có hỗ trợ phương thức synset() không\n",
    "                if hasattr(sense_tag, 'synset'):\n",
    "                    synset = sense_tag.synset()\n",
    "                    # Lấy lemma từ tên synset (ví dụ: \"bank.n.01\" -> \"bank\")\n",
    "                    lemma = synset.name().split('.')[0]\n",
    "                    if lemma == target_lemma:\n",
    "                        # Trích xuất từ và POS từ ngữ cảnh:\n",
    "                        # Lấy 2 từ trước\n",
    "                        wi_m2 = ' '.join(sent[i-2].leaves())\n",
    "                        pos_m2 = sent[i-2].label()\n",
    "                        wi_m1 = ' '.join(sent[i-1].leaves())\n",
    "                        pos_m1 = sent[i-1].label()\n",
    "                        # Lấy 2 từ sau\n",
    "                        wi_p1 = ' '.join(sent[i+1].leaves())\n",
    "                        pos_p1 = sent[i+1].label()\n",
    "                        wi_p2 = ' '.join(sent[i+2].leaves())\n",
    "                        pos_p2 = sent[i+2].label()\n",
    "                        \n",
    "                        # Tạo bigram từ ngữ cảnh bên trái và bên phải\n",
    "                        bigram_left = f\"{wi_m2}_{wi_m1}\"\n",
    "                        bigram_right = f\"{wi_p1}_{wi_p2}\"\n",
    "                        \n",
    "                        # Tạo dictionary đặc trưng với trọng số khác nhau:\n",
    "                        features = {\n",
    "                            f\"Wi-2:{wi_m2}\": 1,       # Từ cách 2 vị trí trước\n",
    "                            f\"POSi-2:{pos_m2}\": 1,     # POS tương ứng\n",
    "                            f\"Wi-1:{wi_m1}\": 2,        # Từ ngay trước (ưu tiên hơn)\n",
    "                            f\"POSi-1:{pos_m1}\": 2,\n",
    "                            f\"Wi+1:{wi_p1}\": 2,        # Từ ngay sau (ưu tiên hơn)\n",
    "                            f\"POSi+1:{pos_p1}\": 2,\n",
    "                            f\"Wi+2:{wi_p2}\": 1,        # Từ cách 2 vị trí sau\n",
    "                            f\"POSi+2:{pos_p2}\": 1,\n",
    "                            f\"bigram_left:{bigram_left}\": 1,   # Bigram bên trái\n",
    "                            f\"bigram_right:{bigram_right}\": 1  # Bigram bên phải\n",
    "                        }\n",
    "                        # Lưu instance: (features, label) với label là tên synset\n",
    "                        instances.append((features, synset.name()))\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    target_lemma = 'bar'  # Thay đổi từ mục tiêu nếu cần\n",
    "    instances = extract_instances(target_lemma)\n",
    "    \n",
    "    if not instances:\n",
    "        print(f\"Không tìm thấy instance cho từ '{target_lemma}'\")\n",
    "        return\n",
    "    \n",
    "    # Tách đặc trưng và nhãn từ các instance\n",
    "    feature_dicts, labels = zip(*instances)\n",
    "    \n",
    "    # Sử dụng DictVectorizer để chuyển đổi các dictionary đặc trưng thành ma trận số\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X = vectorizer.fit_transform(feature_dicts)\n",
    "    \n",
    "    # Áp dụng TF-IDF để cân bằng trọng số các đặc trưng\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X)\n",
    "    \n",
    "    # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Huấn luyện mô hình kNN; sử dụng khoảng cách cosine thường phù hợp với dữ liệu TF-IDF\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Dự đoán trên tập kiểm tra\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy cho từ '{target_lemma}': {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cho từ 'bar': 0.75\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
